'''
Servicio mejorado para interactuar con un Large Language Model (LLM).
Utiliza Gemini AI para comparaciones de productos y consultas tecnol√≥gicas avanzadas.
'''
import logging
from typing import List, Dict, Any, Optional
import google.generativeai as genai

logger = logging.getLogger(__name__)

class LLMService:
    """
    Servicio mejorado para interactuar con Gemini AI.
    Maneja comparaciones de productos y consultas tecnol√≥gicas avanzadas.
    """
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        if not api_key:
            logger.warning("LLMService inicializado sin API key. Funcionalidad limitada.")
            self.model = None
        else:
            try:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("LLMService inicializado correctamente con Gemini AI")
            except Exception as e:
                logger.error(f"Error inicializando Gemini AI: {e}")
                self.model = None

    def generate_comparison_fallback(
        self,
        item1_name: str,
        item2_name: str,
        attributes: List[str],
        item1_data: Optional[Dict[str, Any]] = None,
        item2_data: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Genera una comparaci√≥n detallada usando Gemini AI cuando la b√∫squeda estructurada falla o es insuficiente.
        """
        logger.info(f"LLM Comparaci√≥n: '{item1_name}' vs '{item2_name}' en atributos: {attributes}")
        
        if not self.model:
            return self._fallback_comparison_response(item1_name, item2_name, attributes)
        
        try:
            prompt = self._build_comparison_prompt(item1_name, item2_name, attributes, item1_data, item2_data)
            response = self.model.generate_content(prompt)
            return response.text.strip()
            
        except Exception as e:
            logger.error(f"Error en comparaci√≥n LLM: {e}")
            return self._fallback_comparison_response(item1_name, item2_name, attributes)
    
    def _build_comparison_prompt(self, item1_name: str, item2_name: str, attributes: List[str], 
                                item1_data: Optional[Dict[str, Any]], item2_data: Optional[Dict[str, Any]]) -> str:
        """Construir prompt especializado para comparaci√≥n de productos tecnol√≥gicos"""
        
        prompt = f"""
Eres un experto consultor en tecnolog√≠a de GRUPO INFOTEC, empresa l√≠der en equipos tecnol√≥gicos en Per√∫.

TAREA: Compara detalladamente '{item1_name}' con '{item2_name}' para ayudar a un cliente a tomar la mejor decisi√≥n.

ASPECTOS A COMPARAR:
{', '.join(attributes) if attributes and 'caracteristicas' not in attributes else 'Todas las caracter√≠sticas relevantes'}

INFORMACI√ìN DISPONIBLE:
"""
        
        if item1_data:
            prompt += f"\nüìä DATOS DE {item1_name.upper()}:\n"
            for key, value in item1_data.items():
                if value and value != "N/A":
                    prompt += f"‚Ä¢ {key.replace('_', ' ').title()}: {value}\n"
        
        if item2_data:
            prompt += f"\nüìä DATOS DE {item2_name.upper()}:\n"
            for key, value in item2_data.items():
                if value and value != "N/A":
                    prompt += f"‚Ä¢ {key.replace('_', ' ').title()}: {value}\n"
        
        prompt += f"""

INSTRUCCIONES:
1. Proporciona una comparaci√≥n clara y estructurada
2. Destaca las ventajas y desventajas de cada producto
3. Considera factores como rendimiento, precio, calidad-precio
4. Usa un tono profesional pero amigable
5. Incluye emojis moderadamente para mejor legibilidad
6. Termina con una recomendaci√≥n seg√∫n diferentes tipos de usuario
7. M√°ximo 300 palabras
8. Si falta informaci√≥n espec√≠fica, basa la comparaci√≥n en conocimiento general de estos productos

Formato de respuesta:
üîç **Comparaci√≥n: {item1_name} vs {item2_name}**

[Comparaci√≥n detallada aqu√≠]

üí° **Recomendaci√≥n:**
[Sugerencia seg√∫n tipo de usuario]
"""
        
        return prompt
    
    def _fallback_comparison_response(self, item1_name: str, item2_name: str, attributes: List[str]) -> str:
        """Respuesta de respaldo cuando no est√° disponible el LLM"""
        return f"""üîç **Comparaci√≥n: {item1_name} vs {item2_name}**

Lo siento, en este momento tengo limitaciones para realizar una comparaci√≥n detallada con IA. 

üìã **Lo que puedo hacer:**
‚Ä¢ Buscar especificaciones individuales de cada producto
‚Ä¢ Mostrar precios y disponibilidad actual
‚Ä¢ Conectarte con un asesor especializado

üí° **Te sugiero:**
1. Consultar las especificaciones de cada producto por separado
2. Contactar a nuestro equipo de ventas para una asesor√≠a personalizada
3. Visitar nuestra tienda para ver los productos en persona

¬øTe gustar√≠a que busque informaci√≥n espec√≠fica de alguno de estos productos?"""

    def recommend_top_products(
        self,
        candidate_products: List[Dict[str, Any]],
        user_query: str,
        category: Optional[str] = None,
        use_case: Optional[str] = None,
        count: int = 3
    ) -> str:
        """
        Recomienda los 'count' mejores productos de una lista de candidatos usando LLM.
        """
        logger.info(f"LLM Recomendaci√≥n: Top {count} productos para query: '{user_query}', categor√≠a: {category}, uso: {use_case}")

        if not candidate_products:
            return "Lo siento, no tengo suficientes datos en este momento para hacer una recomendaci√≥n precisa con IA sobre eso."

        prompt = (
            f"Eres un asistente de ventas experto en tecnolog√≠a. Basado en la siguiente lista de productos de nuestra base de datos y la consulta del cliente, "
            f"recomienda los {count} mejores y explica brevemente por qu√© cada uno es una buena opci√≥n.\n"
            f"Consulta del cliente: '{user_query}'\n"
        )
        if category:
            prompt += f"Categor√≠a de inter√©s: {category}\n"
        if use_case:
            prompt += f"Uso principal: {use_case}\n"
        
        prompt += "Productos disponibles (nombre, precio, y otros datos relevantes):\n"
        for i, prod_data in enumerate(candidate_products[:10]): # Limitar a 10 para no exceder el prompt
            simple_prod_data = {
                "nombre": prod_data.get("name"),
                "precio": prod_data.get("price"),
                "descripci√≥n_corta": prod_data.get("description", "")[:100] + "...",
                "rating": prod_data.get("rating", "N/A")
            }
            if 'specifications' in prod_data and isinstance(prod_data['specifications'], dict):
                simple_prod_data.update({k: v for k, v in prod_data['specifications'].items() if k in ['processor_model', 'ram_amount', 'storage_capacity', 'graphics_card_model']})

            prompt += f"{i+1}. {simple_prod_data}\n"
        
        prompt += f"\nPor favor, presenta tu recomendaci√≥n de los {count} mejores de forma clara y convincente."

        # ----- SIMULACI√ìN DE RESPUESTA LLM -----
        response_text = (
            f"¬°Claro! Basado en tu solicitud '{user_query}' y nuestros productos, aqu√≠ est√°n mis {count} recomendaciones principales:\n"
        )
        for i, prod in enumerate(candidate_products[:count]):
            response_text += (
                f"{i+1}. **{prod.get('name', 'Producto Incre√≠ble')}** (Precio: S/ {prod.get('price', 'Consultar')})\n"
                f"   - *Por qu√© es genial*: [Explicaci√≥n generada por IA destacando por qu√© es bueno para '{user_query}'. Esta es una simulaci√≥n.]\n"
            )
        response_text += "\nEspero que esto te ayude a decidir. ¬°Av√≠same si tienes m√°s preguntas!"
        # ----- FIN SIMULACI√ìN -----

        logger.debug(f"LLM Prompt para recomendaci√≥n: {prompt}")
        logger.debug(f"LLM Respuesta simulada: {response_text}")
        return response_text
